{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../<2018-12-02>Outputs-BLACK BOX CODE/\n"
     ]
    }
   ],
   "source": [
    "from my_logging import my_outputs_and_logging\n",
    "\n",
    "log = my_outputs_and_logging('BLACK BOX CODE')\n",
    "\n",
    "print log.get_output_path()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First data Reading from Bulk data and clean it(with lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Articles read -> 180702\n",
      "#Articles read successfully -> 177321\n",
      "#Articles saved successfully -> 177321\n"
     ]
    }
   ],
   "source": [
    "from file_commander import my_file_commander\n",
    "\n",
    "commander = my_file_commander()\n",
    "### TO READ\n",
    "article_path ='/Users/uluc/Desktop/Bitirme/Wikiparse_WorkSpace/<2018.10.-->Wiki/<2018-10-20>Outputs_Bulk/All_Article.txt'\n",
    "splitter_patter = '\\n\\n\\n'\n",
    "dataContains_tuples = ['id','title','infoBox_type','text_infoBox','bulk_paragraph']\n",
    "articles = commander.my_tub_file_reader(article_path,splitter_patter,dataContains_tuples)\n",
    "\n",
    "##3 this part otomaticley parse infobox\n",
    "\n",
    "###Save this clean Data\n",
    "### TO SAVE\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph']\n",
    "commander.my_tub_file_recorder(log.get_output_path()+'All_Articles.txt',articles,'\\n\\n\\n',dataContains_tuples)\n",
    "\n",
    "log.save_log('Read Bulk Data',u'#Articles read -> '+str(commander.read_total)+'\\n#Articles read successfully -> '+str(commander.read_successfully)+'\\n#Articles saved successfully -> '+str(commander.saved_successfully))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Articles saved successfully -> 53115\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sapsik_bir_Analyser import Article_Analyser\n",
    "log.add_splitter()\n",
    "Analysis_Article = Article_Analyser(articles)\n",
    "\n",
    "mypath = log.get_output_path() +'InfoBoxType_Analysis/'\n",
    "\n",
    "# > 4\n",
    "all_categories_greater_4 = Analysis_Article.get_all_uniq_infoBoxTypes_as_list(4)\n",
    "log.logging('#Total Uniq InfoBox Type ( > 4 ) : '+ str(len(all_categories_greater_4)))\n",
    "for one in all_categories_greater_4 : \n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Analysis','all_categories( > 4 )',one.encode('utf-8'))\n",
    "Analysis_Article.draw_Repetition_of_all_InfoBoxTypes(mypath,'all_categories( > 4 )',4)\n",
    "\n",
    "# > 100\n",
    "all_categories_greater_100 = Analysis_Article.get_all_uniq_infoBoxTypes_as_list(100)\n",
    "Analysis_Article.draw_Repetition_of_all_InfoBoxTypes(mypath,'all_categories( > 100 )',100)\n",
    "log.logging('#Total Uniq InfoBox Type ( > 100 ) : '+ str(len(all_categories_greater_100)))\n",
    "for one in all_categories_greater_100 : \n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Analysis','all_categories( > 100 )',one.encode('utf-8'))\n",
    "\n",
    "    \n",
    "# Our interested Info Box Types\n",
    "Interested_Info_Box_Types = ['Hakem' ,'Manken' ,'Makam sahibi' ,'Filozof' ,'Bilim insanı','Güreşçi' \n",
    "                             ,'Bilim adamı' ,'Sporcu' ,'Buz patencisi','Asker' \n",
    "                             ,'Voleybolcu' ,'Sanatçı','Futbolcu' ,'Oyuncu' \n",
    "                             ,'Müzik sanatçısı' ,'Yazar' ,'Kraliyet' ,'Tenis sporcu' ,'Profesyonel güreşçi'\n",
    "                             ,'Kişi' ,'Basketbolcu'] #'Çizgi roman karakteri' , 'Kurgusal karakter'\n",
    "log.save_log('Interested Info Box Types',json.dumps(Interested_Info_Box_Types, ensure_ascii=False, encoding='utf8').encode('utf-8'))\n",
    "\n",
    "Analysis_Article.ignore_other_types(Interested_Info_Box_Types)\n",
    "\n",
    "all_categories_interested = Analysis_Article.get_all_uniq_infoBoxTypes_as_list(100)\n",
    "\n",
    "Analysis_Article.draw_Repetition_of_all_InfoBoxTypes(mypath,'all_categories( > Interested Types )',1)\n",
    "log.logging('#Total Uniq InfoBox Type ( > Interested ) : '+ str(len(all_categories_interested)))\n",
    "log.logging('#Total Article ( > Interested ) : '+ str(len(Analysis_Article.articles)))\n",
    "for one in  all_categories_interested : \n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Analysis','all_categories( > interested )',one.encode('utf-8'))\n",
    "    \n",
    "    \n",
    "###Save for all interested clean data\n",
    "### TO SAVE\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph']\n",
    "commander.my_tub_file_recorder(log.get_output_path()+'All_Articles_Interested.txt',Analysis_Article.articles,'\\n\\n\\n',dataContains_tuples)\n",
    "log.logging('#Total Articles saved successfully ( > Interested ) : '+ str(commander.saved_successfully))\n",
    "\n",
    "## for each info box type one example is saved\n",
    "import json\n",
    "examples = Analysis_Article.get_one_example_for_every_infoBox_type()\n",
    "for type_,one_example, in examples :\n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Examples',type_,json.dumps(one_example, ensure_ascii=False, encoding='utf8',indent=4).encode('utf-8'))\n",
    "log.logging('For each info box type one example is saved')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Data Field Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = log.get_output_path() +'InfoBoxType_DataField_Counts'\n",
    "if not os.path.isdir(mypath):\n",
    "    os.makedirs(mypath)\n",
    "mypath = mypath+'/'    \n",
    "Analysis_Article.count_data_fields()\n",
    "Analysis_Article.save_allCounts_2_file(mypath)\n",
    "Analysis_Article.save_Counts_for_types(mypath)\n",
    "Analysis_Article.save_uniq_fields(mypath)\n",
    "Analysis_Article.save_dataField_Analysis(mypath)     \n",
    "log.logging('All Data Field Countings finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Articles read -> 53116\n",
      "#Articles read successfully -> 53115\n"
     ]
    }
   ],
   "source": [
    "from file_commander import my_file_commander\n",
    "from sapsik_bir_Analyser import Article_Analyser\n",
    "commander = my_file_commander()\n",
    "\n",
    "### TO READ\n",
    "article_path = log.get_output_path()+'All_Articles_Interested.txt'\n",
    "splitter_patter = '\\n\\n\\n'\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph']\n",
    "articles = commander.my_tub_file_reader(article_path,splitter_patter,dataContains_tuples)\n",
    "Analysis_Article = Article_Analyser(articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'replace' BURDAAA\n",
      "'NoneType' object has no attribute 'replace' BURDAAA\n",
      "'NoneType' object has no attribute 'replace' BURDAAA\n",
      "'NoneType' object has no attribute 'replace' BURDAAA\n",
      "'NoneType' object has no attribute 'replace' BURDAAA\n",
      "'NoneType' object has no attribute 'replace' BURDAAA\n",
      "#Tried to convert to DA from infoBox 53115\n",
      "#Converted to DA from infoBox 43657\n",
      "#DA saved succesfully 101621\n",
      "#Articles saved successfully -> 53115\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import DA_Combiner as Combiner\n",
    "rules = Combiner.get_rules('./RULES.txt')\n",
    "Analysis_Article.articles ,possible_DAs = Combiner.create_DA_combinations(rules,Analysis_Article.articles)\n",
    "save_path = log.get_output_path() + 'Dialog_Acts'\n",
    "tried,counter,DA_id = Combiner.save_DAs(save_path,possible_DAs)\n",
    "\n",
    "\n",
    "log.add_splitter()\n",
    "log.save_log('DA Convertion',str( '#Tried to convert to DA from infoBox -> '+str(tried) \n",
    "             + '\\n#Converted to DA from infoBox -> '+ str(counter) \n",
    "             + '\\n#DA saved succesfully -> '+ str(DA_id) ))\n",
    "\n",
    "###Save for all interested clean data\n",
    "### TO SAVE\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph','DA_as_str','DA_fields']\n",
    "commander.my_tub_file_recorder(log.get_output_path()+'All_Articles_Interested_V2.txt',Analysis_Article.articles,'\\n\\n\\n',dataContains_tuples)\n",
    "log.logging('#Total Articles saved successfully ( > Interested_V2 ) : '+ str(commander.saved_successfully))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import json \n",
    "from dataCleaner import process_bulk_paragraph, set_environment, clear_environment\n",
    "\n",
    "# Sample usage of dataCleaner.py\n",
    "\n",
    "set_environment() # create outputs folder to work in with temporary text files\n",
    "\n",
    "# process the bulk data\n",
    "clean_sentence_number = 0 \n",
    "for i,a in enumerate(Analysis_Article.articles):\n",
    "    clean_paragraph ,sentences, AckMessage = process_bulk_paragraph(a.article['bulk_paragraph'])\n",
    "    sentance_str = sentences[0] + '@'+sentences[1]\n",
    "    Analysis_Article.articles[i].article['clean_paragraph'] = clean_paragraph  \n",
    "    Analysis_Article.articles[i].article['clean_sentences'] = sentance_str\n",
    "    if sentences[0] != 'None':\n",
    "        clean_sentence_number +=1\n",
    "\n",
    "\n",
    "clear_environment() # clear all temporary files\n",
    "\n",
    "\n",
    "###Save for all interested clean data\n",
    "### TO SAVE\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph','DA_as_str','DA_fields','clean_paragraph','clean_sentences']\n",
    "commander.my_tub_file_recorder(log.get_output_path()+'All_Articles_Interested_V3.txt',Analysis_Article.articles,'\\n\\n\\n',dataContains_tuples)\n",
    "log.logging('#Total Articles saved successfully ( > Interested_V3 ) : '+ str(commander.saved_successfully))\n",
    "log.logging('#Total Articles saved with Sentence/s ( > Interested_V3 ) : '+ str(clean_sentence_number))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
