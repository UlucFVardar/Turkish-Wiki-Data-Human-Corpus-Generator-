{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../<2018-11-27>Outputs-BLACK BOX CODE/\n"
     ]
    }
   ],
   "source": [
    "from my_logging import my_outputs_and_logging\n",
    "\n",
    "log = my_outputs_and_logging('BLACK BOX CODE')\n",
    "\n",
    "print log.get_output_path()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First data Reading from Bulk data and clean it(with lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Articles read -> 180702\n",
      "#Articles read successfully -> 177321\n",
      "#Articles saved successfully -> 177321\n"
     ]
    }
   ],
   "source": [
    "from file_commander import my_file_commander\n",
    "\n",
    "commander = my_file_commander()\n",
    "### TO READ\n",
    "article_path ='/Users/uluc/Desktop/Bitirme/Wikiparse_WorkSpace/<2018.10.-->Wiki/<2018-10-20>Outputs_Bulk/All_Article.txt'\n",
    "splitter_patter = '\\n\\n\\n'\n",
    "dataContains_tuples = ['id','title','infoBox_type','text_infoBox','bulk_paragraph']\n",
    "articles = commander.my_tub_file_reader(article_path,splitter_patter,dataContains_tuples)\n",
    "\n",
    "##3 this part otomaticley parse infobox\n",
    "\n",
    "###Save this clean Data\n",
    "### TO SAVE\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph']\n",
    "commander.my_tub_file_recorder(log.get_output_path()+'All_Articles.txt',articles,'\\n\\n\\n',dataContains_tuples)\n",
    "\n",
    "log.save_log('Read Bulk Data',u'#Articles read -> '+str(commander.read_total)+'\\n#Articles read successfully -> '+str(commander.read_successfully)+'\\n#Articles saved successfully -> '+str(commander.saved_successfully))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Articles saved successfully -> 53115\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sapsik_bir_Analyser import Article_Analyser\n",
    "log.add_splitter()\n",
    "Analysis_Article = Article_Analyser(articles)\n",
    "\n",
    "mypath = log.get_output_path() +'InfoBoxType_Analysis/'\n",
    "\n",
    "# > 4\n",
    "all_categories_greater_4 = Analysis_Article.get_all_uniq_infoBoxTypes_as_list(4)\n",
    "log.logging('#Total Uniq InfoBox Type ( > 4 ) : '+ str(len(all_categories_greater_4)))\n",
    "for one in all_categories_greater_4 : \n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Analysis','all_categories( > 4 )',one.encode('utf-8'))\n",
    "Analysis_Article.draw_Repetition_of_all_InfoBoxTypes(mypath,'all_categories( > 4 )',4)\n",
    "\n",
    "# > 100\n",
    "all_categories_greater_100 = Analysis_Article.get_all_uniq_infoBoxTypes_as_list(100)\n",
    "Analysis_Article.draw_Repetition_of_all_InfoBoxTypes(mypath,'all_categories( > 100 )',100)\n",
    "log.logging('#Total Uniq InfoBox Type ( > 100 ) : '+ str(len(all_categories_greater_100)))\n",
    "for one in all_categories_greater_100 : \n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Analysis','all_categories( > 100 )',one.encode('utf-8'))\n",
    "\n",
    "    \n",
    "# Our interested Info Box Types\n",
    "Interested_Info_Box_Types = ['Hakem' ,'Manken' ,'Makam sahibi' ,'Filozof' ,'Bilim insanı','Güreşçi' \n",
    "                             ,'Bilim adamı' ,'Sporcu' ,'Buz patencisi','Asker' \n",
    "                             ,'Voleybolcu' ,'Sanatçı','Futbolcu' ,'Oyuncu' \n",
    "                             ,'Müzik sanatçısı' ,'Yazar' ,'Kraliyet' ,'Tenis sporcu' ,'Profesyonel güreşçi'\n",
    "                             ,'Kişi' ,'Basketbolcu'] #'Çizgi roman karakteri' , 'Kurgusal karakter'\n",
    "log.save_log('Interested Info Box Types',json.dumps(Interested_Info_Box_Types, ensure_ascii=False, encoding='utf8').encode('utf-8'))\n",
    "\n",
    "Analysis_Article.ignore_other_types(Interested_Info_Box_Types)\n",
    "\n",
    "all_categories_interested = Analysis_Article.get_all_uniq_infoBoxTypes_as_list(100)\n",
    "\n",
    "Analysis_Article.draw_Repetition_of_all_InfoBoxTypes(mypath,'all_categories( > Interested Types )',1)\n",
    "log.logging('#Total Uniq InfoBox Type ( > Interested ) : '+ str(len(all_categories_interested)))\n",
    "log.logging('#Total Article ( > Interested ) : '+ str(len(Analysis_Article.articles)))\n",
    "for one in  all_categories_interested : \n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Analysis','all_categories( > interested )',one.encode('utf-8'))\n",
    "    \n",
    "    \n",
    "###Save for all interested clean data\n",
    "### TO SAVE\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph']\n",
    "commander.my_tub_file_recorder(log.get_output_path()+'All_Articles_Interested.txt',Analysis_Article.articles,'\\n\\n\\n',dataContains_tuples)\n",
    "log.logging('#Total Articles saved successfully ( > Interested ) : '+ str(commander.saved_successfully))\n",
    "\n",
    "## for each info box type one example is saved\n",
    "import json\n",
    "examples = Analysis_Article.get_one_example_for_every_infoBox_type()\n",
    "for type_,one_example, in examples :\n",
    "    log.create_a_file_in_a_folder('InfoBoxType_Examples',type_,json.dumps(one_example, ensure_ascii=False, encoding='utf8',indent=4).encode('utf-8'))\n",
    "log.logging('For each info box type one example is saved')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Data Field Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = log.get_output_path() +'InfoBoxType_DataField_Counts'\n",
    "if not os.path.isdir(mypath):\n",
    "    os.makedirs(mypath)\n",
    "mypath = mypath+'/'    \n",
    "Analysis_Article.count_data_fields()\n",
    "Analysis_Article.save_allCounts_2_file(mypath)\n",
    "Analysis_Article.save_Counts_for_types(mypath)\n",
    "Analysis_Article.save_uniq_fields(mypath)\n",
    "Analysis_Article.save_dataField_Analysis(mypath)     \n",
    "log.logging('All Data Field Countings finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Articles read -> 53116\n",
      "#Articles read successfully -> 53115\n",
      "'NoneType' object has no attribute 'replace'\n",
      "'NoneType' object has no attribute 'replace'\n",
      "'NoneType' object has no attribute 'replace'\n",
      "'NoneType' object has no attribute 'replace'\n",
      "'NoneType' object has no attribute 'replace'\n",
      "'NoneType' object has no attribute 'replace'\n",
      "#Tried to convert to DA from infoBox 53115\n",
      "#Converted to DA from infoBox 43657\n",
      "#DA saved succesfully 101621\n"
     ]
    }
   ],
   "source": [
    "from file_commander import my_file_commander\n",
    "from sapsik_bir_Analyser import Article_Analyser\n",
    "commander = my_file_commander()\n",
    "\n",
    "### TO READ\n",
    "article_path = log.get_output_path()+'All_Articles_Interested.txt'\n",
    "splitter_patter = '\\n\\n\\n'\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph']\n",
    "articles = commander.my_tub_file_reader(article_path,splitter_patter,dataContains_tuples)\n",
    "Analysis_Article = Article_Analyser(articles)\n",
    "\n",
    "\n",
    "import combiner as Combiner\n",
    "rules = Combiner.get_rules('./RULES.txt')\n",
    "possible_DAs = Combiner.create_DA_combinations(rules,Analysis_Article.articles)\n",
    "save_path = log.get_output_path() + 'Dialog_Acts'\n",
    "tried,counter,DA_id = Combiner.save_DAs(save_path,possible_DAs)\n",
    "\n",
    "log.add_splitter()\n",
    "log.save_log('DA Convertion',str( '#Tried to convert to DA from infoBox -> '+str(tried) \n",
    "             + '\\n#Converted to DA from infoBox -> '+ str(counter) \n",
    "             + '\\n#DA saved succesfully -> '+ str(DA_id) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from my_logging import my_outputs_and_logging\n",
    "\n",
    "log = my_outputs_and_logging('BLACK BOX CODE')\n",
    "#log.save_log('BLACK BOX CODE',u'This Code takes the bulk wiki dump data.\\nThen Parse it to end up with possible dialog acts')\n",
    "\n",
    "print log.get_output_path()\n",
    "\n",
    "from file_commander import my_file_commander\n",
    "from sapsik_bir_Analyser import Article_Analyser\n",
    "commander = my_file_commander()\n",
    "### TO READ\n",
    "article_path = '../<2018-11-26>Outputs-BLACK BOX CODE/All_Articles_Interested.txt'\n",
    "splitter_patter = '\\n\\n\\n'\n",
    "dataContains_tuples = ['id','title','infoBox_type','bulk_infoBox','clean_infoBox','bulk_paragraph']\n",
    "articles = commander.my_tub_file_reader(article_path,splitter_patter,dataContains_tuples)\n",
    "Analysis_Article = Article_Analyser(articles)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str() takes at most 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8f1a02b3c3a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m log.save_log('DA Convertion',str( '#Tried to convert to DA from infoBox -> '+str(tried) \n\u001b[1;32m      2\u001b[0m              \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nConverting to DA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mu'#Converted to DA from infoBox -> '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m              + '\\n#DA saved succesfully -> '+ str(DA_id) ))\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: str() takes at most 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "log.save_log('DA Convertion',str( '#Tried to convert to DA from infoBox -> '+str(tried) \n",
    "             + '\\nConverting to DA',u'#Converted to DA from infoBox -> '+ str(counter) \n",
    "             + '\\n#DA saved succesfully -> '+ str(DA_id) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
