{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Article():\n",
    "    def __init__(self,text):\n",
    "        try:\n",
    "            bulk = text.split('#')\n",
    "            if len(bulk) != 6:\n",
    "                self.kill = True\n",
    "            else:\n",
    "                self.kill = False\n",
    "                self.id = bulk[0]\n",
    "                self.title = bulk[1]\n",
    "                self.info_box_type = bulk[2]\n",
    "                self.clean_info_box = json.loads(bulk[3])\n",
    "                self.bulk_info_box = json.loads(bulk[4])\n",
    "                self.paragraph = bulk[5]\n",
    "                self.filed_counter = Counter(self.clean_info_box)\n",
    "        except Exception as e:\n",
    "            print e\n",
    "    def get_string(self):\n",
    "        s =self.id\n",
    "        s += '#'+self.title\n",
    "        s += '#'+self.info_box_type\n",
    "        s += '#'+self.clean_info_box\n",
    "        s += '#'+self.bulk_info_box\n",
    "        s +=     self.paragraph\n",
    "        return s\n",
    "    def get_string_for_print(self):\n",
    "        s =self.id\n",
    "        s += '#'+self.title\n",
    "        s += '#'+self.info_box_type\n",
    "        s += '#'+json.dumps(self.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\n",
    "        s += '#'+json.dumps(self.bulk_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\n",
    "        s +=     self.paragraph\n",
    "        return s\n",
    "\n",
    "def reader(articles_path):\n",
    "    article_text = open(articles_path, \"r\") .read()\n",
    "    articles = []\n",
    "    for a in article_text.split('\\n\\n\\n'):\n",
    "        article = Article(a)\n",
    "        if article.kill == False:\n",
    "            articles.append(article)\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path ='/Users/uluc/Desktop/Bitirme/Wikiparse_WorkSpace/<2018.10.-->Wiki/<2018-10-21>Outputs-DataFieldCount/All_Article.txt'\n",
    "articles = reader(article_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def d_tarihi(value,effect):\n",
    "    month = ['Ocak','Şubat','Mart','Nisan','Mayıs','Haziran','Temmuz','Ağustos','Eylül','Ekim','Kasım','Aralık']\n",
    "    orj = value\n",
    "\n",
    "    try :\n",
    "        value = re.findall(r\"(\\|\\d+)\", value, re.MULTILINE)\n",
    "        if len(value) == 3:\n",
    "            value = ('%s')% (value[2][1:].encode('ascii','ignore')+' '+month[int(value[1][1:].encode('ascii','ignore'))-1]+' '+value[0][1:].encode('ascii','ignore') )\n",
    "            return value\n",
    "        elif len(value) == 6:\n",
    "            value1 = ('%s')% (value[2][1:].encode('ascii','ignore')+' '+month[int(value[1][1:].encode('ascii','ignore'))-1]+' '+value[0][1:].encode('ascii','ignore') )    \n",
    "            if effect == 3:\n",
    "                value2 =  ('%s')% (value[2+effect][1:].encode('ascii','ignore')+' '+month[int(value[1+effect][1:].encode('ascii','ignore'))-1]+' '+value[0+effect][1:].encode('ascii','ignore') )\n",
    "                \n",
    "                year1 = int(value[0][1:].encode('ascii','ignore'))\n",
    "                year2 = int(value[0+effect][1:].encode('ascii','ignore'))\n",
    "                if year1 >year2:\n",
    "                    return value1\n",
    "                else:\n",
    "                    return value2\n",
    "                \n",
    "            return value1\n",
    "    except Exception as e:\n",
    "        value = orj\n",
    "        \n",
    "    value = orj\n",
    "    try :\n",
    "        if len(value) == 4 or len(value) ==3:\n",
    "            value = str(value)\n",
    "            return value\n",
    "    except Exception as e:\n",
    "        value = orj\n",
    "\n",
    "    value = orj\n",
    "    try :\n",
    "        if value.count('-') >0:\n",
    "            value = value[:value.find('-')]\n",
    "        if 'M.' in value[:2] or 'MS' == value[:2] or 'MÖ' == value[:2]:\n",
    "            return str(value.encode('utf-8'))\n",
    "    except Exception as e:\n",
    "        value = orj\n",
    "\n",
    "    value = orj.replace(',',' ').replace('  ',' ').replace('.',' ')\n",
    "    try :\n",
    "        \n",
    "        if len(value.split(' ')) == 3:\n",
    "            if value.split(' ')[1].encode('utf-8') in month:\n",
    "                return str(value.encode('utf-8'))\n",
    "            if value.split(' ')[1].isdigit():\n",
    "                value = value.split(' ')\n",
    "                value = ('%s')% (value[0].encode('ascii','ignore')+' '+month[int(value[1].encode('ascii','ignore'))-1]+' '+value[2].encode('ascii','ignore') )\n",
    "                return value                \n",
    "    except Exception as e:\n",
    "        value = orj        \n",
    "    return ''\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "for a in articles:\n",
    "    try:\n",
    "        a.clean_info_box['ad'] = a.clean_info_box['ad'].replace('\\\\','').replace('\\\"','')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    if 'doğumtarihi'.decode('utf8') in json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8'):\n",
    "        orj = a.clean_info_box['doğumtarihi'.decode('utf8')]\n",
    "        try:\n",
    "            #del a.clean_info_box['doğumtarihi'.decode('utf8')] \n",
    "            value = d_tarihi(orj,int(0))\n",
    "            a.clean_info_box['doğumtarihi'.decode('utf8')] = value.strip()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "for a in articles:\n",
    "    if 'ölümtarihi'.decode('utf8') in json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8'):\n",
    "        orj = a.clean_info_box['ölümtarihi'.decode('utf8')]\n",
    "        try:\n",
    "            #del a.clean_info_box['ölümtarihi'.decode('utf8')] \n",
    "            value = d_tarihi(orj,int(3))\n",
    "            a.clean_info_box['ölümtarihi'.decode('utf8')] = value.strip()\n",
    "        except Exception as e:\n",
    "            pass     \n",
    "for a in articles:\n",
    "    for k in a.clean_info_box.keys():\n",
    "        if a.clean_info_box[k] == u'':\n",
    "            del a.clean_info_box[k]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor a in articles[:800]:\\n    if 'Futbolcu' in a.info_box_type:\\n    #if u'Mumcu' in json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8'):\\n        #print json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\\n        try:\\n            #print json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\\n            #print json.dumps(a.bulk_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\\n            #print a.bulk_info_box['do\\xc4\\x9fumtarihi'.decode('utf8')]\\n            pass\\n        except Exception as e:\\n            pass\\n        #print a.paragraph,'\\n\\n'\\n        #dc = DataCleaner()\\n        #dc.process_bulk_paragraph( a.paragraph )\\n        #print dc.clean_sentence,'\\n'\\n        \\n        #print a.paragraph\\n        \\n        #print '--------'\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for a in articles[:800]:\n",
    "    if 'Futbolcu' in a.info_box_type:\n",
    "    #if u'Mumcu' in json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8'):\n",
    "        #print json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\n",
    "        try:\n",
    "            #print json.dumps(a.clean_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\n",
    "            #print json.dumps(a.bulk_info_box,indent = 4,ensure_ascii=False, encoding='utf8')\n",
    "            #print a.bulk_info_box['doğumtarihi'.decode('utf8')]\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        #print a.paragraph,'\\n\\n'\n",
    "        #dc = DataCleaner()\n",
    "        #dc.process_bulk_paragraph( a.paragraph )\n",
    "        #print dc.clean_sentence,'\\n'\n",
    "        \n",
    "        #print a.paragraph\n",
    "        \n",
    "        #print '--------'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_for_print(self):\n",
    "    s = self.id\n",
    "    s += '#'+self.title\n",
    "    s += '#'+self.info_box_type\n",
    "    s += '#'+json.dumps(self.clean_info_box,indent = 4)\n",
    "    s += '#'+json.dumps(self.bulk_info_box,indent = 4)\n",
    "    s += self.paragraph\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"do\\u011fum_tarihi\": \"[[1162]]\", \n",
      "    \"resim_boyutu\": \"\", \n",
      "    \"tam ismi\": \"Temu\\u00e7in\", \n",
      "    \"dini\": \"[[Tengricilik|G\\u00f6ktengricilik]]\", \n",
      "    \"sonra_gelen\": \"[[\\u00d6geday|\\u00d6geday Han]]\", \n",
      "    \"\\u00e7ocuklar\\u0131\": \"[[Cuci]]<br />[[\\u00c7a\\u011fatay]]<br />[[\\u00d6geday]]<br />[[Tuluy]]\", \n",
      "    \"altyaz\\u0131\": \"\", \n",
      "    \"hanedan\": \"[[B\\u00f6r\\u00e7igin]]\", \n",
      "    \"ta\\u00e7_giymesi\": \"\", \n",
      "    \"babas\\u0131\": \"[[Yes\\u00fcgey]]\", \n",
      "    \"h\\u00fck\\u00fcm_s\\u00fcresi\": \"1206 \\u2013 17 A\\u011fustos 1227\", \n",
      "    \"imza\": \"\", \n",
      "    \"do\\u011fum_yeri\": \"D\\u00fcl\\u00fcn-Boldak, [[Mo\\u011folistan]]\", \n",
      "    \"ba\\u015fl\\u0131k\": \"[[Han]]<br/>[[Ka\\u011fan]]\", \n",
      "    \"veliaht\": \"\", \n",
      "    \"e\\u015fi\": \"[[B\\u00f6rte|B\\u00f6rte Ujin]] <br> Kulan <br> Yusui <br> Yusigen\", \n",
      "    \"\\u00f6l\\u00fcm_yeri\": \"\", \n",
      "    \"defin_yeri\": \"\", \n",
      "    \"veraset\": \"1. [[Mo\\u011fol \\u0130mparatorlu\\u011fu|Mo\\u011fol \\u0130mparatorlu\\u011fu Han\\u0131]]\", \n",
      "    \"defin_tarihi\": \"\", \n",
      "    \"resim\": \"YuanEmperorAlbumGenghisPortrait.jpg\", \n",
      "    \"\\u00f6nce_gelen\": \"\", \n",
      "    \"annesi\": \"[[H\\u00f6elin]]\", \n",
      "    \"\\u00f6l\\u00fcm_tarihi\": \"{{\\u00d6l\\u00fcm tarihi ve ya\\u015f\\u0131|1227|8|18|1162|1|1}}\", \n",
      "    \"isim\": \"Cengiz Han<br />\\u0427\\u0438\\u043d\\u0433\\u0438\\u0441 \\u0425\\u0430\\u0430\\u043d\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for a in articles[:1]:\n",
    "    print json.dumps(a.bulk_info_box,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def create_files():\n",
    "    today = date.today().strftime('<%Y-%m-%d>')\n",
    "    mypath = '../'+today+'Outputs-DataFieldClean'\n",
    "    if not os.path.isdir(mypath):\n",
    "        os.makedirs(mypath)\n",
    "\n",
    "\n",
    "    '''    \n",
    "    article_path = '../'+today+'Outputs-InfoBoxClean/'+today+'All_Article.txt'\n",
    "    counter_path = '../'+today+'Outputs-InfoBoxClean/'+today+'infoBoxType_Counter.txt'\n",
    "    log_path = '../'+today+'Outputs-InfoBoxClean/'+today+'Extractor Report.txt'\n",
    "    f= open(article_path,\"w\")\n",
    "    f= open(counter_path,\"w\")\n",
    "    f= open(log_path,\"w\")\n",
    "    '''\n",
    "    return mypath+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = create_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path+'Clean_Interested_Articles_V2.txt','ab')\n",
    "for aa in articles:\n",
    "    a = aa.id, aa.title,aa.info_box_type,aa.clean_info_box, aa.bulk_info_box, aa.paragraph\n",
    "    bd = json.dumps(a[3], ensure_ascii=False, encoding='utf8').encode('utf-8')\n",
    "    f.write(str(a[0]) +'#')\n",
    "    f.write(str(a[1]) +'#')\n",
    "    f.write(str(a[2]) +'#')\n",
    "    f.write(bd)\n",
    "    f.write('#')\n",
    "    #f.write(cd)\n",
    "    #f.write('#')\n",
    "    f.write(str(a[5])) \n",
    "    f.write('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
